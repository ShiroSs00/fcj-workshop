[{"uri":"https://ShiroSs00.github.io/fcj-workshop/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Ho Tran Quan\nPhone Number: 0962401194\nEmail: quanhtse180175@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/5-workshop/5.1-introduction/","title":"Introduction","tags":[],"description":"","content":"Architecture Amazon ECR (Elastic Container Registry) Amazon ECR is a fully managed Docker image registry that allows you to securely store, manage, and version your container images. It integrates seamlessly with ECS, CodeBuild, and CodePipeline.\nAWS CodePipeline AWS CodePipeline is a fully managed CI/CD service that automates the software release process. Whenever changes are pushed to the source repository, CodePipeline triggers stages such as Source → Build → Deploy without manual intervention.\nAWS CodeBuild AWS CodeBuild is a fully managed build service that compiles source code, runs tests, builds Docker images, tags them, and pushes them to ECR. All build steps are defined in a simple buildspec.yml file.\nWorkshop Overview In this workshop, you will learn how to build an automated CI/CD pipeline for containerized applications using AWS CodePipeline, CodeBuild, and Amazon ECR. You will create a complete workflow that retrieves source code, builds a Docker image, and pushes it securely to an ECR repository. Throughout the workshop, you will configure essential AWS services, work with buildspec files, and understand how each component contributes to a smooth and automated deployment process.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Virtual Production on the Cloud: Grup Mediaprо Unleashes Creativity with AWS Virtual production is revolutionizing the media and entertainment industry. Grup Mediaprо, a leading Spanish production company, has successfully leveraged AWS cloud services to transform their production workflows and unlock new creative possibilities.\nThe Evolution of Media Production Traditional media production faces significant challenges:\nPhysical Set Construction: Time-consuming and expensive Location Shooting: Complex logistics and high costs Limited Creative Flexibility: Difficult to make changes during production Geographic Constraints: Bound to available locations Resource Limitations: Fixed infrastructure capacity Virtual production changes this paradigm by combining real-time graphics, LED volume stages, and cloud computing to create immersive digital environments.\nUnderstanding Virtual Production Virtual production enables content creators to:\nCreate Digital Environments: Build any imaginable setting digitally Real-Time Visualization: See final results during filming Iterate Quickly: Make creative changes instantly Reduce Production Time: Eliminate setup and breakdown time Lower Costs: Reduce physical infrastructure requirements Increase Safety: Eliminate dangerous location work The technology combines LED volume stages (large screens displaying real-time graphics) with motion-capture and tracking systems to create seamless integration between live actors and digital backgrounds.\nGrup Mediaprо\u0026rsquo;s Challenge As a major Spanish production company handling numerous productions simultaneously, Grup Mediaprо needed:\nScalable Infrastructure: Support multiple concurrent productions High-Performance Rendering: Real-time 4K/8K graphics processing Global Collaboration: Enable teams across different locations Cost Efficiency: Reduce overall production expenses Flexibility: Adapt to diverse client requirements Reliability: Ensure 24/7 production availability Building this on-premises would require massive capital investment and complex management.\nAWS-Powered Virtual Production Solution Architecture Foundation The solution leverages AWS services across multiple layers:\nRendering \u0026amp; Compute:\nAmazon EC2 GPU Instances: High-performance GPU instances (g4dn, g3s) for real-time rendering AWS Batch: Process complex rendering jobs efficiently Amazon Elastic Graphics: Attach graphics acceleration to instances Auto Scaling: Dynamically adjust capacity based on workload Storage \u0026amp; Asset Management:\nAmazon S3: Store video assets, project files, 3D models, and textures Amazon EBS: High-performance block storage for active projects AWS DataSync: Efficiently transfer large files between on-premises and cloud AWS Backup: Protect against data loss with automated backups Media Processing \u0026amp; Delivery:\nAWS Elemental MediaConvert: Convert video between different formats AWS Elemental MediaPackage: Package video for various delivery formats AWS Elemental MediaLive: Process and encode live video streams Amazon CloudFront: Globally distributed content delivery network AWS Direct Connect: Dedicated network connection for high-bandwidth transfers Content Management \u0026amp; Collaboration:\nAmazon AppStream 2.0: Stream graphics applications to remote artists AWS Wickr Enterprise: Secure team communication Amazon QuickSight: Create dashboards and reports on production metrics Amazon WorkSpaces: Provide remote desktop access for team members Database \u0026amp; Metadata:\nAmazon DynamoDB: Store project metadata and real-time production data Amazon RDS: Manage relational data for project management Amazon Elasticache: Cache frequently accessed data Security \u0026amp; Compliance:\nAWS Identity and Access Management (IAM): Fine-grained access control Amazon VPC: Isolated network environment AWS KMS: Encryption key management CloudTrail: Audit and compliance logging Implementation Workflow Project Initiation Client requirements captured in project management system 3D environments and assets uploaded to S3 Rendering specifications defined Pre-Production Assets processed and optimized for real-time rendering LED volume content pre-rendered using AWS Batch for complex scenes Team members access files via AppStream 2.0 for remote collaboration Production Real-time rendering on EC2 GPU instances Content streamed to LED volume in studio via CloudFront Multiple concurrent scenes rendered simultaneously On-demand scaling handles sudden complexity spikes Post-Production Raw video captured from studio floors uploaded to S3 MediaConvert transforms video for different platforms Team collaborates on edits using cloud-based tools Final content delivered via CloudFront CDN Archive \u0026amp; Analytics Completed projects archived in S3 Glacier for long-term storage Production metrics analyzed in QuickSight dashboards Lessons learned documented for future projects Key Benefits Achieved Production Efficiency Setup Time: Reduced from days to hours Iteration Speed: Changes implemented in minutes vs. hours Production Schedule: Compress timelines by 30-50% Resource Utilization: Maximize use of studio time Cost Optimization No Physical Sets: Eliminate construction and disposal costs Location Expenses: No travel or location permits required Equipment Ownership: Pay-per-use cloud model instead of capital purchases Staffing Flexibility: Scale team size based on project needs Overall Savings: 40-60% reduction in production costs Creative Capabilities Unlimited Environments: Create any digital setting imaginable Rapid Prototyping: Test multiple creative options quickly Real-Time Feedback: Client sees final results during production No Physical Constraints: Impossible camera movements now possible Consistency: Perfectly consistent backgrounds across takes Scalability \u0026amp; Reliability Simultaneous Productions: Run multiple projects on shared infrastructure Global Reach: Support international teams and collaborators Auto-Scaling: Automatically expand capacity during peak demand High Availability: Built-in redundancy ensures continuous operation On-Demand Growth: Add capacity instantly without procurement Technical Achievements Metric Achievement Rendering Speed Real-time 4K at 60fps Resolution Support Up to 8K for specific shots Scene Complexity Support for millions of polygons Asset Storage Petabyte-scale repository Global Delivery \u0026lt;50ms latency globally Concurrent Productions 20+ simultaneous projects Team Size 100+ remote collaborators Uptime 99.99% availability Best Practices for Virtual Production on AWS 1. Infrastructure Design Use GPU-optimized instances for rendering workloads Implement multi-region deployment for disaster recovery Design for elasticity and auto-scaling Use dedicated connections for video transfer 2. Asset Management Organize assets hierarchically in S3 Implement versioning for project files Use metadata tagging for easy discovery Archive completed projects to Glacier 3. Performance Optimization Pre-process heavy assets before real-time use Use CloudFront for efficient content delivery Cache frequently used 3D models and textures Optimize network bandwidth usage 4. Cost Management Use Spot Instances for batch rendering Schedule off-peak rendering for non-urgent tasks Monitor and optimize instance types Use Reserved Instances for baseline capacity 5. Security \u0026amp; Compliance Encrypt all data in transit and at rest Implement least privilege access policies Enable audit logging for all activities Regular security assessments and updates 6. Team Collaboration Use AppStream 2.0 for remote artist access Implement centralized file management Create automated backup and recovery processes Establish clear access and permission policies AWS Services Utilized Service Role EC2 with GPU Real-time rendering Elastic Graphics GPU acceleration S3 Asset and project storage EBS High-performance storage Elemental MediaConvert Video format conversion Elemental MediaPackage Video packaging Elemental MediaLive Live video processing CloudFront Content delivery AppStream 2.0 Remote graphics application access DynamoDB Project metadata RDS Project management database Batch Batch rendering jobs DataSync Data transfer QuickSight Analytics and reporting VPC Network isolation IAM Access management KMS Encryption management CloudTrail Audit logging Impact on the Industry Grup Mediaprо\u0026rsquo;s success with AWS-powered virtual production demonstrates:\nViability of Cloud Production: Enterprise-grade productions can run entirely in cloud Economic Benefits: Significant cost reduction compared to traditional methods Creative Freedom: Digital environments enable unprecedented creative possibilities Market Competitiveness: Ability to compete with international studios Team Empowerment: Remote collaboration enables access to global talent Future Vision With this foundation, Grup Mediaprо plans to:\nAI-Powered Scene Generation: Machine learning for automated environment creation Real-Time Collaboration: Multiple directors and artists working simultaneously Extended Reality (XR): Support for VR and AR content creation Advanced Analytics: Deeper insights into production metrics and workflows Blockchain Integration: Digital rights management and asset tracking Conclusion By embracing AWS cloud services, Grup Mediaprо has successfully transformed from traditional production methods to cutting-edge virtual production. The scalable, flexible, and cost-effective cloud infrastructure enables the company to:\nDeliver higher-quality content faster Reduce production costs significantly Attract and retain top creative talent Expand into new markets and projects Maintain competitive advantage in evolving media industry Virtual production on AWS represents the future of media creation—where creativity is limited only by imagination, not by physical infrastructure or budget constraints. Grup Mediaprо\u0026rsquo;s success story serves as a blueprint for other production companies seeking to innovate and transform their operations.\nThe convergence of cloud computing, real-time graphics, and collaborative tools creates unprecedented opportunities for the media and entertainment industry to reimagine content creation.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Contribute Content to AWS Using NDI with AWS Elemental MediaConnect Network Device Interface (NDI) technology has revolutionized how media content creators can share and distribute video feeds across networks. When combined with AWS Elemental MediaConnect, NDI enables seamless integration of remote content sources into AWS-based production workflows. This blog explores how organizations can leverage NDI and AWS services to create flexible, scalable media contribution workflows.\nUnderstanding NDI Technology Network Device Interface (NDI) is a modern video codec standard that allows devices to share video feeds over standard networks (IP networks) instead of requiring expensive specialized hardware connections.\nKey Characteristics of NDI:\nIP-Based: Uses standard network infrastructure instead of specialized video cables Low Latency: Provides real-time video transmission with minimal delay Bandwidth Efficient: Optimized compression reduces bandwidth requirements Discovery: Automatic device discovery on networks Interoperable: Works across multiple platforms and devices Cost-Effective: Eliminates need for expensive specialized hardware Common NDI Applications:\nRemote broadcasting from multiple locations Live event coverage with distributed camera crews Virtual studio production with remote participants News gathering and distribution Educational content creation with remote instructors Corporate communication and live streaming AWS Elemental MediaConnect Overview AWS Elemental MediaConnect is a high-quality video transport service that securely sends live video feeds into AWS. It enables flexible video ingest from multiple sources and provides features for video routing, failover, and quality monitoring.\nMediaConnect Key Features:\nFlexible Input Options: Supports multiple video input protocols including RTMP, RTP, and ZIXI High Availability: Automatic failover between redundant sources Quality Monitoring: Real-time monitoring of video quality and network performance Entitlements: Control which users can view and route specific video feeds Encryption: Secure video transport with encryption options Scalability: Handle multiple concurrent video feeds Integration: Seamless integration with other AWS media services NDI with MediaConnect: The Integration Why Combine NDI and MediaConnect? Integrating NDI sources with AWS Elemental MediaConnect creates a powerful combination:\nFlexible Content Sources: Accept video from NDI-enabled devices worldwide Cloud-Native Workflows: Leverage AWS services for processing and distribution Reduced Costs: Use standard network infrastructure instead of expensive specialized lines Scalability: Easily add new sources as production needs grow Professional Quality: Maintain broadcast-quality video throughout the workflow Architecture Components Source Layer (On-Premises or Remote):\nNDI cameras and video equipment NDI software applications Network connectivity to AWS NDI converters that transform video to AWS-compatible formats AWS Ingest Layer:\nAWS Elemental MediaConnect for video input Network ingestion endpoints Video monitoring and quality checks Processing Layer:\nAWS Elemental MediaConvert for format conversion AWS Elemental MediaLive for live processing and encoding AWS Lambda for custom processing logic AWS Elemental MediaPackage for packaging Storage and Delivery Layer:\nAmazon S3 for archived content Amazon CloudFront for content distribution AWS Elemental MediaPackage for streaming delivery Monitoring and Management:\nAmazon CloudWatch for operational monitoring AWS X-Ray for distributed tracing Custom dashboards for production visibility Implementation Guide Step 1: Configure NDI Sources On-premises Setup:\n1. Install NDI tools or use NDI-compatible equipment 2. Ensure network connectivity to AWS (via Direct Connect or public internet) 3. Configure NDI to stream video in compatible format 4. Test network bandwidth and latency Network Requirements:\nMinimum bandwidth: 50 Mbps for HD content Recommended bandwidth: 100+ Mbps for reliable delivery Latency: \u0026lt;100ms for optimal performance Network stability: Consistent connection without dropouts Step 2: Set Up AWS Elemental MediaConnect Create MediaConnect Flow:\n1. Create MediaConnect input - Specify input type (RTMP, RTP, ZIXI) - Configure security groups for network access - Enable encryption if required 2. Configure entitlements - Define which outputs can receive the stream - Set access control policies 3. Add outputs - Route to MediaLive for processing - Route to MediaConvert for format conversion - Route to archive destinations IAM Permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;mediaconnect:CreateFlow\u0026#34;, \u0026#34;mediaconnect:DescribeFlow\u0026#34;, \u0026#34;mediaconnect:UpdateFlow\u0026#34;, \u0026#34;mediaconnect:ListFlows\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 3: Process NDI Content Using MediaLive for Live Processing:\nCreate channel for real-time transcoding Apply graphics overlays Insert advertisements Create multiple output qualities Using MediaConvert for File-Based Processing:\nConvert formats for different platforms Create multiple quality versions Optimize for specific delivery channels Step 4: Distribute Content Live Streaming:\nUse MediaPackage for packaging Enable multi-bitrate adaptive streaming Distribute via CloudFront for global reach On-Demand Content:\nArchive to S3 Create content catalog Serve through CloudFront Real-World Use Cases Case 1: Remote News Gathering Scenario: News organization gathering stories from multiple locations simultaneously\nWorkflow:\nField reporters use NDI cameras at remote locations Video streams sent to AWS via MediaConnect MediaLive processes feeds in real-time Master control room monitors all feeds Selected feeds distributed to broadcasters Content archived for later use Benefits:\nEliminates expensive satellite trucks Enable reporters in any location with internet Reduce transmission costs by 70% Enable real-time monitoring from central location Case 2: Virtual Event Production Scenario: Corporate event with speakers and participants across multiple countries\nWorkflow:\nSpeakers use NDI-compatible equipment at their locations Feeds ingested via MediaConnect MediaLive composites multiple feeds Adding graphics, transitions, and effects Distribution via CloudFront for global audience Recording to S3 for on-demand playback Benefits:\nSupport global participation Professional production quality Scalable infrastructure Cost-effective alternative to physical events Case 3: Sports Broadcasting Scenario: Live sports event with multiple camera angles from stadium\nWorkflow:\nStadium NDI cameras feed multiple angles MediaConnect ingests all feeds MediaLive switches between cameras Apply graphics for score displays Distribute to broadcasters and social media Archive complete game for highlight generation Benefits:\nMultiple camera angles without additional infrastructure Real-time switching capability Archive for highlight generation Reduce on-site equipment and personnel Best Practices 1. Network Planning Ensure adequate bandwidth with headroom Implement redundancy for critical feeds Monitor network performance continuously Use Direct Connect for consistent quality 2. Quality Monitoring Monitor video metrics in real-time Set up CloudWatch alarms for quality issues Implement automatic failover for failed sources Regularly test redundancy mechanisms 3. Security Implementation Encrypt video in transit and at rest Implement strong access control with IAM Use security groups to restrict network access Enable VPC endpoints for private connectivity Regular security audits and updates 4. Cost Optimization Choose appropriate instance types for processing Use Reserved Capacity for baseline needs Implement auto-scaling for variable demand Archive old content to S3 Glacier for cost savings 5. Operational Excellence Document workflows and procedures Implement comprehensive monitoring Create runbooks for common scenarios Regular training for operations teams Maintain redundant systems for critical operations Technical Considerations Latency Management Component Typical Latency NDI Network Transport 10-50ms MediaConnect Ingestion 20-100ms MediaLive Processing 100-500ms CloudFront Distribution 10-100ms Total End-to-End 150-750ms Bandwidth Requirements Video Format Bitrate HD (1080p) @ 30fps 3-8 Mbps HD (1080p) @ 60fps 6-12 Mbps 4K (2160p) @ 30fps 12-25 Mbps 4K (2160p) @ 60fps 25-50 Mbps Scaling Capabilities Concurrent Sources: 100+ simultaneous NDI feeds Output Distribution: Scale to millions of simultaneous viewers Processing Throughput: Handle complex transcoding for multiple formats Storage Capacity: Petabyte-scale archival in S3 AWS Services Integration Service Role MediaConnect Video ingest and routing MediaLive Real-time video processing MediaConvert File-based transcoding MediaPackage Packaging for streaming S3 Content storage and archival CloudFront Global content distribution CloudWatch Monitoring and alerting IAM Access control and security VPC Network isolation Direct Connect Dedicated network connection Troubleshooting Guide Common Issues and Solutions Issue: High Latency or Buffering\nSolution: Check network bandwidth and reduce video bitrate Solution: Use AWS Direct Connect for dedicated connection Solution: Enable multiple redundant sources Issue: Video Quality Degradation\nSolution: Monitor CloudWatch metrics for network issues Solution: Adjust compression settings in NDI source Solution: Check MediaConnect flow statistics Issue: Source Connection Failures\nSolution: Verify network connectivity and firewall rules Solution: Check security group configurations Solution: Implement automatic failover to backup sources Issue: Processing Delays\nSolution: Scale MediaLive resources Solution: Simplify processing rules Solution: Use appropriate instance types Conclusion Combining NDI technology with AWS Elemental MediaConnect creates a flexible, scalable, and cost-effective solution for modern media production. Organizations can:\nEliminate Geographic Barriers: Contribute content from anywhere in the world Reduce Infrastructure Costs: Use standard network infrastructure instead of specialized equipment Scale Flexibly: Add sources and processing capacity on-demand Maintain Professional Quality: Leverage AWS services for broadcast-quality output Ensure Reliability: Implement redundancy and failover mechanisms As media production continues to evolve toward cloud-native workflows, the combination of NDI and AWS services provides a powerful foundation for innovation and growth. Whether for news gathering, live events, corporate communications, or entertainment production, this integration offers the flexibility and scalability needed for modern content creation.\nThe future of media production is cloud-native, flexible, and accessible—and NDI with AWS Elemental MediaConnect makes it a reality today.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Dynamically Configuring Job Settings with AWS Elemental MediaConvert AWS Elemental MediaConvert is a powerful video transcoding service that enables media organizations to convert video content into multiple formats optimized for different platforms and devices. While basic transcoding is straightforward, many production workflows require dynamic configuration of job settings based on input content characteristics, business rules, or customer requirements. This blog explores strategies for dynamically configuring MediaConvert jobs to create flexible, scalable video processing pipelines.\nUnderstanding AWS Elemental MediaConvert AWS Elemental MediaConvert is a file-based video transcoding service designed for video-on-demand (VOD) workflows. It converts video content from source formats into multiple output formats suitable for different delivery platforms.\nKey Features:\nFlexible Input/Output: Supports 40+ input and output video formats Quality Processing: Industry-leading video quality at lower bitrates Batch Processing: Process multiple files efficiently Advanced Features: Motion graphics, audio mixing, caption handling Scalability: Handle unlimited concurrent transcoding jobs Cost-Effective: Pay only for the content you process Common Use Cases:\nVideo-on-demand content preparation Multi-quality format creation for adaptive streaming Archive format conversion Content normalization across platforms Live event post-processing and archival The Challenge: Static vs. Dynamic Configuration Traditional Static Configuration Most organizations start with static job templates that define fixed output specifications. While simple to implement, static configurations have limitations:\nLimitations:\nCannot adapt to different input video characteristics One-size-fits-all approach may waste resources Difficult to accommodate diverse content types Manual adjustments required for special cases Limited ability to optimize for business requirements Example Static Scenario: All videos encoded at 1080p/30fps at 5 Mbps regardless of:\nOriginal resolution (could be 720p or 4K) Original frame rate (24fps, 25fps, 60fps) Content type (animation vs. live action) Target platforms (mobile, web, TV) Dynamic Configuration Approach Dynamic configuration adapts transcoding parameters based on input content and business rules, providing:\nAdvantages:\nOptimizes quality for each unique input Reduces unnecessary processing costs Enables content-aware encoding Supports multiple delivery profiles Automates complex decision-making Scales efficiently across diverse content Architecture for Dynamic MediaConvert Configuration High-Level Workflow Input Analysis ↓ Extract Metadata ↓ Apply Business Rules ↓ Generate Job Configuration ↓ Submit MediaConvert Job ↓ Monitor \u0026amp; Track ↓ Archive \u0026amp; Distribute Components 1. Input Monitoring\nS3 event notifications trigger workflow CloudWatch Events monitor source bucket EventBridge routes events to processing pipeline 2. Content Analysis\nLambda function analyzes input video properties MediaInfo or FFmpeg extracts detailed metadata Determines resolution, frame rate, codec, bitrate 3. Decision Engine\nAWS Step Functions orchestrates workflow Evaluates metadata against business rules Selects appropriate output profiles Determines quality parameters 4. Job Configuration\nBuilds dynamic MediaConvert job definition Applies quality presets based on analysis Configures multiple outputs for different platforms Sets up error handling and notifications 5. Job Submission\nSubmits configured job to MediaConvert Tracks job progress and status Implements retry logic for failures 6. Post-Processing\nMove completed files to distribution locations Update content catalog/database Trigger downstream workflows Generate delivery manifests Implementation Strategies Strategy 1: Resolution-Based Configuration Adapt output quality based on input resolution to avoid upscaling.\nDecision Logic:\nIf input resolution \u0026gt;= 4K (2160p): - Create 4K, 1080p, 720p outputs - Use higher bitrates for quality preservation Else if input resolution \u0026gt;= 1080p: - Create 1080p, 720p, 480p outputs - Standard bitrate allocation Else if input resolution \u0026gt;= 720p: - Create 720p, 480p, 360p outputs - Lower bitrates appropriate for source Else (\u0026lt; 720p): - Create single output at source resolution - Minimal bitrate to avoid quality loss Benefits:\nPrevents unnecessary upscaling artifacts Optimizes bitrate allocation Reduces processing for lower-resolution content Improves storage and delivery efficiency Strategy 2: Frame Rate Normalization Handle variable input frame rates intelligently.\nDecision Logic:\nExtract input frame rate If frame rate == 23.976 fps (film): - Output at 24fps or 25fps - Apply appropriate pulldown If frame rate == 29.97 fps (NTSC): - Output at 30fps standard If frame rate == 59.94 fps (NTSC 60): - Output at 60fps - Create 30fps variant for compatibility If frame rate is variable: - Normalize to closest standard - Flag for manual review if uncertain Benefits:\nHandles broadcast frame rates correctly Creates compatible outputs for all platforms Prevents playback timing issues Reduces file size where possible Strategy 3: Content-Type-Based Configuration Different content types require different encoding strategies.\nContent Detection:\nAnalyze content characteristics: - Scene complexity (histogram analysis) - Motion detection (optical flow) - Bitrate consistency (texture analysis) - Black frames (scene detection) Classify as: - Animated/Simple (lower bitrate adequate) - Standard video (moderate bitrate) - High-motion sports/action (higher bitrate) - Talking-head/static (lower bitrate) Encoding Parameters:\nAnimated Content: - Lower bitrate (2-4 Mbps for 1080p) - Moderate GOP size (8-10 seconds) - Simple encoding profile Live Action: - Standard bitrate (5-8 Mbps for 1080p) - Balanced GOP size - Balanced encoding High-Motion: - Higher bitrate (8-12 Mbps for 1080p) - Shorter GOP (6-8 seconds) - Advanced encoding profile Strategy 4: Multi-Platform Output Configuration Generate optimized outputs for different delivery platforms.\nPlatform Profiles:\nWeb (Responsive): - 1920x1080 @ 6 Mbps H.264 - 1280x720 @ 3 Mbps H.264 - 854x480 @ 1.5 Mbps H.264 Mobile (Bandwidth-Constrained): - 1280x720 @ 2.5 Mbps H.264 - 854x480 @ 1 Mbps H.264 - 640x360 @ 500 kbps H.264 TV/Premium (High-Quality): - 3840x2160 @ 20 Mbps HEVC - 1920x1080 @ 8 Mbps HEVC - 1280x720 @ 4 Mbps HEVC Social Media (Quick Share): - 1280x720 @ 2.5 Mbps - Square format 1080x1080 @ 2.5 Mbps - Vertical 1080x1920 @ 2.5 Mbps Building the Decision Engine with AWS Step Functions Workflow Definition { \u0026#34;StartAt\u0026#34;: \u0026#34;AnalyzeInput\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;AnalyzeInput\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:region:account:function:AnalyzeVideo\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;EvaluateRules\u0026#34; }, \u0026#34;EvaluateRules\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:region:account:function:EvaluateBusinessRules\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;SelectProfile\u0026#34; }, \u0026#34;SelectProfile\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.resolution\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;4K\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;Configure4KOutput\u0026#34; }, { \u0026#34;Variable\u0026#34;: \u0026#34;$.resolution\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;1080p\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;Configure1080pOutput\u0026#34; }, { \u0026#34;Variable\u0026#34;: \u0026#34;$.resolution\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;720p\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;Configure720pOutput\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;ConfigureDefaultOutput\u0026#34; }, \u0026#34;Configure4KOutput\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:region:account:function:Build4KJobDefinition\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;SubmitJob\u0026#34; }, \u0026#34;Configure1080pOutput\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:region:account:function:Build1080pJobDefinition\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;SubmitJob\u0026#34; }, \u0026#34;Configure720pOutput\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:region:account:function:Build720pJobDefinition\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;SubmitJob\u0026#34; }, \u0026#34;ConfigureDefaultOutput\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:region:account:function:BuildDefaultJobDefinition\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;SubmitJob\u0026#34; }, \u0026#34;SubmitJob\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::mediaconvert:createJob.sync\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;JobSuccess\u0026#34; }, \u0026#34;JobSuccess\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Succeed\u0026#34; } } } Analysis Lambda Function import json import boto3 import subprocess mediaconvert = boto3.client(\u0026#39;mediaconvert\u0026#39;) def lambda_handler(event, context): bucket = event[\u0026#39;detail\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;detail\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] # Get file information s3_uri = f\u0026#34;s3://{bucket}/{key}\u0026#34; # Extract metadata using ffprobe metadata = extract_metadata(s3_uri) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;bucket\u0026#39;: bucket, \u0026#39;key\u0026#39;: key, \u0026#39;resolution\u0026#39;: metadata[\u0026#39;resolution\u0026#39;], \u0026#39;frameRate\u0026#39;: metadata[\u0026#39;frameRate\u0026#39;], \u0026#39;codec\u0026#39;: metadata[\u0026#39;codec\u0026#39;], \u0026#39;bitrate\u0026#39;: metadata[\u0026#39;bitrate\u0026#39;], \u0026#39;duration\u0026#39;: metadata[\u0026#39;duration\u0026#39;], \u0026#39;contentType\u0026#39;: classify_content(metadata) } def extract_metadata(s3_uri): # Use ffprobe to get detailed metadata cmd = [ \u0026#39;ffprobe\u0026#39;, \u0026#39;-v\u0026#39;, \u0026#39;error\u0026#39;, \u0026#39;-select_streams\u0026#39;, \u0026#39;v:0\u0026#39;, \u0026#39;-show_entries\u0026#39;, \u0026#39;stream=width,height,r_frame_rate,codec_name,bit_rate\u0026#39;, \u0026#39;-of\u0026#39;, \u0026#39;csv=p=0\u0026#39;, s3_uri ] output = subprocess.check_output(cmd).decode(\u0026#39;utf-8\u0026#39;) parts = output.strip().split(\u0026#39;,\u0026#39;) width, height = int(parts[0]), int(parts[1]) resolution = determine_resolution(width, height) return { \u0026#39;resolution\u0026#39;: resolution, \u0026#39;width\u0026#39;: width, \u0026#39;height\u0026#39;: height, \u0026#39;frameRate\u0026#39;: float(parts[2]), \u0026#39;codec\u0026#39;: parts[3], \u0026#39;bitrate\u0026#39;: int(parts[4]) if parts[4] else 0, \u0026#39;duration\u0026#39;: get_duration(s3_uri) } def determine_resolution(width, height): if width \u0026gt;= 3840 or height \u0026gt;= 2160: return \u0026#39;4K\u0026#39; elif width \u0026gt;= 1920 or height \u0026gt;= 1080: return \u0026#39;1080p\u0026#39; elif width \u0026gt;= 1280 or height \u0026gt;= 720: return \u0026#39;720p\u0026#39; else: return \u0026#39;SD\u0026#39; def classify_content(metadata): # Simplified classification # In production, use more sophisticated analysis if metadata[\u0026#39;bitrate\u0026#39;] \u0026lt; 1000000: # \u0026lt; 1 Mbps return \u0026#39;animation\u0026#39; elif metadata[\u0026#39;frameRate\u0026#39;] \u0026gt; 50: return \u0026#39;highmotion\u0026#39; else: return \u0026#39;standard\u0026#39; Real-World Use Cases Use Case 1: Video Content Platform Scenario: Streaming platform receives user-uploaded content\nWorkflow:\nUser uploads video System analyzes video properties Based on resolution and duration: Determine appropriate quality ladder Select encoding profiles Generate multiple outputs for adaptive streaming Create thumbnails and preview images Update content database Result: Each video optimized for its characteristics, reducing storage and delivery costs.\nUse Case 2: News and Media Organization Scenario: Multiple content sources with varying specifications\nWorkflow:\nVideo ingested from broadcast feed, satellite feed, or social media Analyze incoming video format and quality Normalize to organization standards Create archive version and broadcast-ready versions Automatically distribute to appropriate platforms Result: Consistent output quality across diverse sources.\nUse Case 3: Live Event Archival Scenario: Live broadcast needs immediate on-demand availability\nWorkflow:\nLive stream captured and recorded Post-broadcast analysis of recorded file Optimize encoding based on actual content Create multiple quality variants Update VOD catalog Distribute to platforms Result: On-demand content ready quickly with optimal quality/size tradeoff.\nBest Practices for Dynamic Configuration 1. Metadata Extraction Use reliable tools (FFmpeg, MediaInfo) Cache results to avoid re-analysis Handle errors gracefully Store metadata for future reference 2. Business Rules Engine Keep rules version-controlled and documented Implement A/B testing for new rules Monitor rule effectiveness and adjust Allow manual overrides when needed 3. Job Configuration Use job templates as base Programmatically modify only what\u0026rsquo;s necessary Validate configurations before submission Implement dry-run capability 4. Error Handling Implement retry logic with exponential backoff Log detailed error information Create fallback configurations Alert on repeated failures 5. Monitoring and Optimization Track metrics: processing time, cost, quality Monitor encoder utilization Identify optimization opportunities Continuously refine rules based on data 6. Cost Optimization Right-size outputs based on content Avoid unnecessary quality levels Use tiered pricing for different content types Archive non-critical versions to Glacier AWS Services Integration Service Purpose S3 Source and output storage EventBridge Trigger workflow on file upload Lambda Content analysis and rule evaluation Step Functions Orchestrate workflow MediaConvert Perform video transcoding CloudWatch Monitor metrics and logs SNS Send notifications DynamoDB Store configuration rules CloudFormation Infrastructure as code Performance Considerations Processing Efficiency Factor Impact Input Resolution Higher resolution requires more processing Duration Longer videos take proportionally longer Output Count Each output adds processing time Complexity More outputs = longer job time Cost Optimization Cost Factors: - Processing time (largest cost component) - Output resolution (higher = higher cost) - Number of outputs (linear scaling) - Premium features (advanced encoding, captions) Cost Reduction: - Analyze input to avoid unnecessary outputs - Skip outputs not needed for distribution - Use efficient codecs (HEVC vs H.264) - Schedule transcoding during off-peak hours Troubleshooting and Monitoring Common Issues Issue: Incorrect Resolution Detection\nSolution: Validate metadata extraction Solution: Implement manual override capability Solution: Review sample outputs for accuracy Issue: Job Configuration Errors\nSolution: Validate job definition before submission Solution: Implement schema validation Solution: Test with sample files Issue: Performance Bottlenecks\nSolution: Monitor Step Functions execution time Solution: Parallelize independent tasks Solution: Optimize metadata extraction Issue: Unexpected Costs\nSolution: Track output configurations Solution: Monitor number of jobs and outputs Solution: Review and optimize rules regularly Conclusion Dynamically configuring AWS Elemental MediaConvert jobs enables organizations to:\nOptimize Quality: Adapt encoding to input content characteristics Reduce Costs: Eliminate unnecessary processing and storage Scale Efficiently: Handle diverse content types automatically Improve Automation: Reduce manual intervention and errors Enable Innovation: Support new platforms and formats easily By combining AWS Elemental MediaConvert with Lambda, Step Functions, and intelligent decision engines, media organizations can build sophisticated, scalable video processing pipelines that adapt to business needs and content characteristics.\nThe future of video transcoding is intelligent, automated, and optimized—making MediaConvert\u0026rsquo;s dynamic configuration capabilities essential for modern media workflows.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “​AI/ML/GenAI on AWS” Summary Report: \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; Event Information Date: Saturday, November 15, 2025 Time: 8:30 AM – 12:00 PM Location: AWS Vietnam Office Event Objectives Introduce AWS AI/ML services landscape in Vietnam Provide hands-on knowledge on Amazon SageMaker for end-to-end ML workflows Explore Generative AI capabilities with Amazon Bedrock Share best practices in prompt engineering and Retrieval-Augmented Generation (RAG) Demonstrate building production-ready AI applications Event Agenda 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-end ML platform Data preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough 10:30 – 10:45 AM | Coffee Break 10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – comparison \u0026amp; selection guide Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base integration Bedrock Agents: Multi-step workflows and tool integrations Guardrails: Safety and content filtering Live Demo: Building a Generative AI chatbot using Bedrock Key Highlights Amazon SageMaker: End-to-End ML Platform Fully managed service for building, training, and deploying machine learning models Data preparation: Automated data labeling and preprocessing Model training: Hyperparameter tuning, distributed training MLOps integration: Model monitoring, versioning, and continuous deployment SageMaker Studio: Unified IDE for data scientists and ML engineers Foundation Models \u0026amp; Model Selection Claude: Advanced reasoning, long-context understanding, instruction-following Llama: Open-source efficiency, customization capabilities Titan: AWS-optimized models for specific use cases Selection criteria: Task type, latency requirements, cost considerations Prompt Engineering Techniques Chain-of-Thought (CoT): Break complex problems into step-by-step reasoning Few-shot learning: Provide examples to guide model behavior Zero-shot prompting: Direct instructions without examples Temperature and parameters: Fine-tuning model output characteristics Retrieval-Augmented Generation (RAG) Architecture: Query → Retrieval → Context → Generation Knowledge Base integration: Connect to enterprise data sources Semantic search: Finding relevant context from large datasets Reducing hallucinations: Grounding responses in actual data Bedrock Agents \u0026amp; Automation Multi-step workflows: Orchestrating complex AI-driven tasks Tool integration: Connecting to APIs, databases, and external services Autonomous decision-making: Agents executing tasks without human intervention Enterprise workflow automation: Improving efficiency and reducing manual work Safety \u0026amp; Guardrails Content filtering: Preventing harmful or inappropriate outputs PII protection: Masking sensitive information Compliance: Meeting regulatory requirements Custom policies: Organization-specific safety rules Key Takeaways Technical Insights SageMaker ecosystem: Comprehensive solution for entire ML lifecycle Foundation Models: Leverage pre-trained models for faster time-to-value RAG pattern: Combine generative AI with retrieval for accurate, contextual responses Bedrock Agents: Build intelligent automation without extensive AI expertise Best Practices Phased approach: Start with simple use cases before complex automation Prompt optimization: Invest time in prompt engineering for better results Data quality: Foundation for successful ML and AI applications Safety first: Implement guardrails and monitoring from the start Business Impact Accelerated development: Pre-built models reduce time-to-market Cost efficiency: Pay-per-token pricing with managed services Scalability: AWS infrastructure handles varying loads automatically Innovation: Enable new use cases previously not feasible Applying to Work Implement SageMaker: Evaluate current ML pipelines for migration opportunities Adopt Bedrock: Pilot generative AI features in existing applications RAG implementation: Build knowledge-aware AI systems for business use cases Prompt engineering: Develop prompt templates and best practices for the team Bedrock Agents: Automate routine workflows using multi-step AI agents Safety measures: Establish guardrails and monitoring for production deployments Event Experience Attending the \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop provided invaluable exposure to cutting-edge AI technologies and practical implementation strategies.\nLearning from AWS experts Gained comprehensive understanding of AWS AI/ML service portfolio Learned real-world best practices for deploying generative AI in production Understood the strengths and trade-offs of different foundation models Hands-on technical exposure SageMaker Studio demo showed the complete ML development workflow Bedrock chatbot demo demonstrated rapid prototyping of generative AI applications Learned practical prompt engineering techniques for better AI output Understood RAG architecture for building accurate, context-aware AI systems Key insights gained Foundation models are accessible and cost-effective for enterprise use Prompt engineering is a critical skill for maximizing AI system performance RAG pattern solves hallucination problems by grounding responses in data Bedrock Agents can automate complex multi-step business processes Practical applications Can implement SageMaker for data-driven ML projects Bedrock can accelerate development of AI-powered features RAG patterns apply to knowledge management and Q\u0026amp;A systems Guardrails ensure responsible AI deployment Networking opportunities Connected with AWS specialists and fellow participants Discussed AI/ML challenges and solutions with peers Gained insights into Vietnam\u0026rsquo;s growing AI/ML landscape Lessons learned Start with pre-trained foundation models before building custom models Invest in prompt engineering for significant performance improvements Implement safety measures and monitoring from project inception Phased approach reduces risk and accelerates time-to-value Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;DevOps on AWS\u0026rdquo; Event Information Date: Monday, November 17, 2025 Time: 8:30 AM – 5:00 PM Location: AWS Vietnam Office Event Objectives Understand DevOps culture, principles, and key metrics Master AWS CI/CD services (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) Learn Infrastructure as Code with CloudFormation and AWS CDK Explore containerization with Docker, ECR, ECS, and EKS Implement monitoring and observability solutions Discover DevOps best practices and real-world case studies Event Agenda Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset Recap of AI/ML session DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline Source Control: AWS CodeCommit, Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates Orchestration: CodePipeline automation Live Demo: Full CI/CD pipeline walkthrough 10:30 – 10:45 AM | Break 10:45 AM – 12:00 PM | Infrastructure as Code (IaC) AWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support Demo: Deploying with CloudFormation and CDK Discussion: Choosing between IaC tools Lunch Break (12:00 – 1:00 PM) Self-arranged lunch break\nAfternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS Docker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison 2:30 – 2:45 PM | Break 2:45 – 4:00 PM | Monitoring \u0026amp; Observability CloudWatch: Metrics, logs, alarms, and dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Full-stack observability setup Best Practices: Alerting, dashboards, and on-call processes 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies Deployment strategies: Feature flags, A/B testing Automated testing and CI/CD integration Incident management and postmortems Case Studies: Startups and enterprise DevOps transformations 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up DevOps career pathways AWS certification roadmap Key Highlights DevOps Culture \u0026amp; Metrics DORA Metrics: Deployment frequency, lead time, mean time to recovery (MTTR), change failure rate Key Benefits: Faster time-to-market, improved reliability, enhanced team collaboration Cultural shift: Breaking silos between development and operations teams Continuous improvement: Iterative processes and feedback loops AWS CI/CD Pipeline Services CodeCommit: Fully managed Git repository with branch protection and pull request reviews CodeBuild: Fully managed build service supporting multiple languages and frameworks CodeDeploy: Automated deployments with multiple strategies (Blue/Green, Canary, Rolling) CodePipeline: Orchestrate workflows across build, test, and deployment stages Git Strategies: GitFlow for feature branches, Trunk-based for continuous deployment Infrastructure as Code (IaC) CloudFormation Templates: JSON/YAML format defining AWS resources Stacks: Collections of resources managed as a single unit Change Sets: Preview changes before applying Drift Detection: Identify manual changes to stack resources Stack Policies: Control who can modify resources AWS CDK Language Support: Python, JavaScript, Java, C#, Go Constructs: Pre-built components for common AWS patterns Reusability: Share constructs across teams and projects Type Safety: Catch errors at development time Synthesis: Generate CloudFormation templates from code Container Services Docker \u0026amp; Containerization Microservices: Breaking monolithic applications into smaller, manageable services Portability: Consistent behavior across different environments Resource Efficiency: Lightweight compared to virtual machines Isolation: Each container runs independently with its dependencies Amazon ECR (Elastic Container Registry) Image Storage: Centralized repository for Docker images Scanning: Automatic vulnerability detection Lifecycle Policies: Automate image retention and cleanup Integration: Seamless integration with ECS, EKS, and other AWS services ECS \u0026amp; EKS ECS (Elastic Container Service): AWS-native orchestration, simpler setup EKS (Elastic Kubernetes Service): Kubernetes-based, more complex but powerful Deployment Strategies: Blue/Green, rolling updates, canary deployments Auto Scaling: Scale based on metrics (CPU, memory, custom metrics) Service Discovery: Automatic load balancing and DNS AWS App Runner Simplified Deployment: Deploy containers without managing infrastructure Automatic Scaling: Handles scaling based on demand Built-in Security: TLS termination, automatic patching Quick Setup: Deploy from source code or container image Monitoring \u0026amp; Observability CloudWatch Metrics: Track application and infrastructure performance Logs: Centralized log aggregation and analysis Alarms: Set thresholds and trigger automated actions Dashboards: Visualize metrics and logs in real-time Custom Metrics: Track application-specific metrics AWS X-Ray Distributed Tracing: Track requests across multiple services Performance Insights: Identify bottlenecks and latency issues Service Map: Visualize service dependencies and interactions Error Analysis: Trace failures to their root causes Integration: Works with Lambda, API Gateway, RDS, and more DevOps Best Practices Feature Flags: Deploy code without releasing features immediately A/B Testing: Compare different implementations in production Automated Testing: Unit, integration, and end-to-end tests in pipeline Incident Management: Documented processes for handling outages Postmortems: Learn from incidents without blame On-Call Rotation: Fair distribution of incident response responsibilities Observability First: Monitor and alert on important metrics Key Takeaways Technical Skills CI/CD Mastery: Build complete automation pipelines with CodePipeline IaC Proficiency: Choose between CloudFormation and CDK for infrastructure Containerization: Understand Docker and orchestration platforms (ECS, EKS) Observability: Implement comprehensive monitoring with CloudWatch and X-Ray Deployment Strategies: Safely roll out changes with minimal risk Best Practices Automation First: Automate everything from builds to deployments Infrastructure as Code: Treat infrastructure like software with versioning Observability Over Monitoring: Understand system behavior, not just metrics Fail Fast, Recover Quick: Implement canary and blue/green deployments Shared Responsibility: Foster collaboration between development and operations Career Insights DevOps Roles: Site Reliability Engineer, Platform Engineer, Infrastructure Engineer AWS Certifications: DevOps Engineer – Professional, Solutions Architect Continuous Learning: DevOps evolves rapidly; stay updated with latest practices Skills in Demand: Automation, cloud platforms, containerization, observability Applying to Work Implement CI/CD: Migrate existing deployment processes to CodePipeline Adopt IaC: Convert manual infrastructure setup to CloudFormation or CDK Containerize Applications: Evaluate which applications benefit from containerization Deploy to ECS/EKS: Choose appropriate container orchestration platform Setup Observability: Implement CloudWatch dashboards and X-Ray tracing Define Deployment Strategies: Implement blue/green or canary deployments Automate Testing: Integrate automated tests into CI/CD pipeline Establish On-Call Process: Implement incident response procedures Event Experience Attending the \u0026ldquo;DevOps on AWS\u0026rdquo; workshop provided comprehensive understanding of modern DevOps practices and AWS services for automation and infrastructure management.\nLearning from AWS experts Gained deep knowledge of AWS DevOps service ecosystem Learned proven strategies for CI/CD pipeline design and implementation Understood different approaches to infrastructure management Discovered best practices for monitoring production systems Hands-on technical exposure CI/CD Pipeline Demo: Saw complete automation from code commit to production deployment IaC Comparison: Understood trade-offs between CloudFormation and CDK Container Orchestration: Learned differences between ECS and EKS Full-stack Observability: Saw how CloudWatch and X-Ray work together Key insights gained DevOps is about culture and process, not just tools Automation reduces manual errors and accelerates releases Infrastructure as Code enables reproducible, version-controlled environments Observability is critical for understanding production system behavior Container orchestration simplifies scaling and deployment of microservices Practical applications Can implement CodePipeline for automated deployments Can use CloudFormation or CDK for infrastructure provisioning Can containerize applications and deploy to ECS or EKS Can setup comprehensive monitoring with CloudWatch and X-Ray Can implement safe deployment strategies (blue/green, canary) Networking opportunities Connected with AWS DevOps specialists Discussed automation challenges with fellow participants Learned about DevOps transformation case studies from enterprise perspective Lessons learned Start with CI/CD automation before complex infrastructure Choose IaC tool based on team expertise and project needs Invest in observability early to reduce troubleshooting time Implement safety mechanisms (feature flags, gradual rollouts) for production changes Foster collaboration between development and operations teams Event Photos Add your event photos here\nOverall, this DevOps workshop significantly enhanced my understanding of production deployment, automation, and infrastructure management. The combination of expert guidance, live demonstrations, and case studies provided actionable insights I can immediately apply to improve development workflows and system reliability.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/1-explore/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 09/10/2025 09/10/2025 https://000001.awsstudygroup.com/ 5 - Cost Management with AWS Budgets: + Cost Budget + Usage Budget + Reservation Budget + Savings Plans Budget 09/11/2025 09/11/2025 https://000007.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 09/12/2025 09/12/2025 https://docs.aws.amazon.com/ec2/ Week 1 Achievements: Understood AWS fundamentals and basic service categories:\nCompute services: EC2, Lambda, Lightsail Storage services: S3, EBS, Glacier Database services: DynamoDB, RDS, Aurora Networking services: VPC, CloudFront, Route 53 Developer Tools: CodePipeline, CodeBuild, CodeDeploy Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Successfully completed EC2 hands-on practice:\nLaunched EC2 t2.micro instance (Free Tier eligible) Created and configured Security Groups for SSH, HTTP, HTTPS Generated and downloaded EC2 key pair Connected to EC2 instance via SSH Attached EBS volume and mounted filesystem Installed and configured web server "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn about IAM (Identity and Access Management) for secure access control. Understand VPC (Virtual Private Cloud) and network architecture. Practice hands-on IAM and VPC configuration. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn about IAM fundamentals + Users + Groups + Roles + Policies + Permissions 09/15/2025 09/15/2025 https://docs.aws.amazon.com/iam/ 2 - Practice IAM: + Create IAM users + Create IAM groups + Attach policies to users and groups + Test user permissions 09/16/2025 09/16/2025 AWS IAM Tutorial 3 - Learn about VPC fundamentals + VPC concept and structure + CIDR blocks and IP allocation + Subnets and availability zones + Public and private subnets 09/17/2025 09/17/2025 https://docs.aws.amazon.com/vpc/ 4 - Learn about Security Groups and Network ACLs + Security Groups: inbound and outbound rules + Network ACLs for additional network security + Stateful vs stateless filtering 09/18/2025 09/18/2025 AWS VPC Security Guide 5 - Practice VPC: + Create custom VPC with CIDR block + Create public and private subnets + Configure Security Groups + Create Internet Gateway 09/19/2025 09/19/2025 AWS VPC Tutorial 6 - Practice VPC (continued): + Set up routing tables and routes + Test connectivity between resources + Verify security group rules and network configuration 09/20/2025 09/20/2025 AWS VPC Tutorial Week 2 Achievements: Understood IAM (Identity and Access Management) fundamentals:\nIAM users for individual user access IAM groups for managing multiple users together IAM roles for service-to-service communication IAM policies and permissions structure Principle of least privilege for security best practices Successfully completed IAM hands-on practice:\nCreated IAM users with different access levels Organized users into logical groups Attached AWS managed and inline policies to users and groups Generated and managed access keys for programmatic access Tested IAM policies by logging in as different users Understood policy conditions and resource restrictions Configured password policies and MFA requirements Understood VPC (Virtual Private Cloud) fundamentals:\nVPC concept: isolated network environment within AWS CIDR blocks for IP address space allocation Subnets: dividing VPC into smaller networks Availability zones for high availability and fault tolerance Public and private subnets for different resource types Route tables for directing network traffic Understood network security concepts:\nSecurity Groups: virtual firewalls for EC2 instances Inbound and outbound rules for traffic control Network ACLs (NACLs) for subnet-level security Stateful vs stateless firewall filtering Default security group behavior "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about S3 (Simple Storage Service) for object storage. Understand DynamoDB (NoSQL database) and its core concepts. Practice hands-on S3 and DynamoDB operations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Learn about S3 fundamentals: buckets, objects, storage classes, versioning, lifecycle policies 09/22/2025 09/22/2025 https://docs.aws.amazon.com/s3/ 2 Practice S3 Lab: Create bucket, upload objects, enable versioning, configure lifecycle policies 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 3 Learn about S3 security: bucket policies, ACLs, pre-signed URLs, encryption 09/24/2025 09/24/2025 https://000057.awsstudygroup.com/ 4 Learn about DynamoDB fundamentals: tables, items, partition key, sort key, capacity modes 09/25/2025 09/25/2025 https://docs.aws.amazon.com/dynamodb/ 5 Practice DynamoDB Lab: Create table, insert items, query and scan operations, auto-scaling 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ 6 Practice DynamoDB (continued): Update items, delete items, configure indexes, TTL settings 09/27/2025 09/27/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Successfully learned and practiced S3 fundamentals:\nCreated S3 buckets and uploaded/downloaded objects Enabled versioning for data protection Configured lifecycle policies for cost optimization Implemented S3 security with bucket policies and ACLs Generated pre-signed URLs for temporary access Enabled server-side encryption for sensitive data Successfully learned and practiced DynamoDB fundamentals:\nCreated DynamoDB tables with partition and sort keys Inserted, updated, and deleted items Performed Query and Scan operations with filters Configured provisioned capacity and auto-scaling Created Global Secondary Indexes for flexible querying Set up TTL for automatic item expiration Monitored table metrics using CloudWatch Gained proficiency in managing object storage and NoSQL databases for production applications.\nIntegrated S3 and DynamoDB knowledge with previous AWS services (EC2, IAM, VPC) for comprehensive infrastructure understanding.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn about Lambda (serverless computing) and event-driven architecture. Understand API Gateway for creating REST APIs. Practice CloudWatch for monitoring and logging. Integrate multiple AWS services into a complete application. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Learn about Lambda fundamentals: functions, triggers, runtime, execution role, permissions 09/29/2025 09/29/2025 https://docs.aws.amazon.com/lambda/ 2 Practice Lambda Lab: Create function, test invocation, configure trigger, set environment variables 09/30/2025 09/30/2025 https://cloudjourney.awsstudygroup.com/ 3 Learn about API Gateway: REST APIs, endpoints, methods, request/response mapping, authorization 10/01/2025 10/01/2025 https://docs.aws.amazon.com/apigateway/ 4 Practice API Gateway Lab: Create REST API, configure resources and methods, integrate with Lambda 10/02/2025 10/02/2025 https://cloudjourney.awsstudygroup.com/ 5 Learn about CloudWatch: monitoring, logging, metrics, alarms, dashboards 10/03/2025 10/03/2025 https://docs.aws.amazon.com/cloudwatch/ 6 Practice CloudWatch Lab: Create logs, set metrics, configure alarms, build dashboards for monitoring 10/04/2025 10/04/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Understood serverless computing with Lambda and event-driven architecture.\nSuccessfully created and tested Lambda functions with proper execution roles and permissions.\nBecame familiar with API Gateway and learned how to create REST APIs and integrate with Lambda.\nInstalled and configured CloudWatch for monitoring, logging, and creating alarms.\nUsed CloudWatch to perform monitoring operations such as:\nView Lambda function logs Create custom metrics Set up CloudWatch alarms Build monitoring dashboards Analyze logs with CloudWatch Logs Insights Successfully completed integration project combining:\nLambda for serverless computation API Gateway for REST API endpoints S3 for object storage triggers DynamoDB for data persistence CloudWatch for end-to-end monitoring Acquired the ability to design event-driven applications and build scalable serverless solutions on AWS.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Practice and reinforce IAM, VPC, S3, and DynamoDB through hands-on labs. Build projects integrating storage and database services. Day Task Start Date Completion Date Reference Material 1 Practice IAM: Create users, roles, groups, and policies 10/06/2025 10/06/2025 https://cloudjourney.awsstudygroup.com/2-manage/2-1-iam/ 2 Practice VPC: Create VPC, subnets, security groups, route tables 10/07/2025 10/07/2025 https://cloudjourney.awsstudygroup.com/2-manage/2-2-vpc/ 3 Practice S3: Create buckets, upload objects, versioning, lifecycle policies 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com/3-store/3-1-s3/ 4 Practice DynamoDB: Create tables, CRUD operations, indexes 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/3-store/3-2-dynamodb/ 5 Project: File storage system with S3 and metadata in DynamoDB 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/3-store/ 6 Troubleshooting: Fix connectivity, permissions, and performance issues 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Successfully practiced IAM user and role creation with proper permission assignments. Built VPC with public and private subnets with working security groups. Created S3 buckets with versioning and lifecycle policies configured. Created DynamoDB tables and performed CRUD operations. Built file storage project combining S3 and DynamoDB. Debugged and fixed connectivity and permission issues. "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Practice Lambda, API Gateway, and CloudWatch through hands-on labs. Build serverless applications integrating multiple services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Practice Lambda: Create functions, configure triggers, test invocations 10/13/2025 10/13/2025 https://cloudjourney.awsstudygroup.com/4-compute/4-2-lambda/ 2 Practice API Gateway: Create REST APIs, integrate with Lambda, test endpoints 10/14/2025 10/14/2025 https://cloudjourney.awsstudygroup.com/5-network/5-1-api-gateway/ 3 Practice CloudWatch: Create logs, metrics, alarms, dashboards 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/6-monitor/6-1-cloudwatch/ 4 Project: User API with Lambda, API Gateway, DynamoDB backend 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/4-compute/ 5 Project: Serverless file processor with S3 events, Lambda, and DynamoDB 10/17/2025 10/17/2025 https://cloudjourney.awsstudygroup.com/4-compute/ 6 Optimization: Performance tuning, cost optimization, monitoring setup 10/18/2025 10/18/2025 https://cloudjourney.awsstudygroup.com/6-monitor/ Week 6 Achievements: Successfully created Lambda functions with multiple trigger types. Built REST APIs with API Gateway and integrated with Lambda. Implemented CloudWatch logging and monitoring for applications. Built user management API with complete CRUD operations. Built file processing workflow triggered by S3 events. Optimized Lambda performance and configured cost monitoring. "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Practice CloudFormation for infrastructure as code. Practice EC2 advanced features including Auto Scaling and Load Balancing. Build automated, scalable infrastructure using CloudFormation templates. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Practice CloudFormation: Write YAML templates, create and update stacks 10/20/2025 10/20/2025 https://cloudjourney.awsstudygroup.com/7-infrastructure/7-1-cloudformation/ 2 Practice CloudFormation: Template parameters, outputs, conditions 10/21/2025 10/21/2025 https://cloudjourney.awsstudygroup.com/7-infrastructure/7-1-cloudformation/ 3 Practice EC2: Auto Scaling groups, launch templates, scaling policies 10/22/2025 10/22/2025 https://cloudjourney.awsstudygroup.com/4-compute/4-1-ec2/ 4 Practice Load Balancing: Application Load Balancer, Elastic Load Balancing 10/23/2025 10/23/2025 https://cloudjourney.awsstudygroup.com/5-network/5-2-load-balancing/ 5 Project: Deploy multi-tier application with CloudFormation (VPC, EC2, RDS, ALB) 10/24/2025 10/24/2025 https://cloudjourney.awsstudygroup.com/7-infrastructure/ 6 Testing: Test auto scaling, load balancer failover, stack updates 10/25/2025 10/25/2025 https://cloudjourney.awsstudygroup.com/7-infrastructure/ Week 7 Achievements: Successfully wrote CloudFormation templates for infrastructure components. Deployed and updated CloudFormation stacks. Configured EC2 Auto Scaling with appropriate scaling policies. Set up Application Load Balancer with health checks. Deployed complete multi-tier infrastructure with CloudFormation. Tested auto scaling and failover scenarios. "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Practice CI/CD pipelines with CodePipeline, CodeBuild, and CodeDeploy. Practice monitoring and logging at scale with CloudWatch and X-Ray. Build and deploy complete applications with automated pipelines. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Practice CodePipeline: Create pipeline, configure stages, integrate with GitHub 10/27/2025 10/27/2025 https://cloudjourney.awsstudygroup.com/8-devops/8-1-cicd/ 2 Practice CodeBuild: Create build projects, configure build specs, test builds 10/28/2025 10/28/2025 https://cloudjourney.awsstudygroup.com/8-devops/8-1-cicd/ 3 Practice CodeDeploy: Configure deployments to EC2, on-premises servers 10/29/2025 10/29/2025 https://cloudjourney.awsstudygroup.com/8-devops/8-1-cicd/ 4 Project: CI/CD pipeline for web application (Source → Build → Deploy) 10/30/2025 10/30/2025 https://cloudjourney.awsstudygroup.com/8-devops/ 5 Practice advanced monitoring: X-Ray tracing, distributed tracing, performance analysis 10/31/2025 10/31/2025 https://cloudjourney.awsstudygroup.com/6-monitor/6-2-xray/ 6 Integration: Complete end-to-end application with deployment automation and monitoring 11/01/2025 11/01/2025 https://cloudjourney.awsstudygroup.com/8-devops/ Week 8 Achievements: Successfully created CodePipeline with multiple stages. Configured CodeBuild projects with custom build specifications. Set up CodeDeploy for automated application deployment. Built CI/CD pipeline for automated build and deployment. Implemented distributed tracing with X-Ray. Deployed and monitored complete application with full automation. "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn ASP.NET Core fundamentals for building REST APIs. Understand Model-View-Controller (MVC) and Dependency Injection patterns. Practice building and testing RESTful APIs with ASP.NET Core. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Learn ASP.NET Core basics: project structure, controllers, routing 11/03/2025 11/03/2025 https://learn.microsoft.com/en-us/aspnet/core/ 2 Learn about Dependency Injection and middleware in ASP.NET Core 11/04/2025 11/04/2025 https://learn.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection 3 Practice: Create ASP.NET Core project, setup controllers, define routes 11/05/2025 11/05/2025 https://learn.microsoft.com/en-us/aspnet/core/tutorials/ 4 Learn about Entity Framework Core and database operations 11/06/2025 11/06/2025 https://learn.microsoft.com/en-us/ef/core/ 5 Practice: Create API endpoints for CRUD operations with EF Core 11/07/2025 11/07/2025 https://learn.microsoft.com/en-us/aspnet/core/tutorials/first-web-api 6 Practice: Test API endpoints, implement error handling, validation 11/08/2025 11/08/2025 https://learn.microsoft.com/en-us/aspnet/core/web-api/ Week 9 Achievements: Understood ASP.NET Core fundamentals and project structure:\nStartup configuration and middleware pipeline Controller-based and minimal API patterns Routing and endpoint mapping Request/response handling Successfully learned Dependency Injection pattern:\nService registration in ConfigureServices Constructor injection for loose coupling Singleton, Scoped, Transient lifetimes Dependency Injection container usage Successfully created ASP.NET Core REST API project:\nSet up API controllers with appropriate routing Defined HTTP GET, POST, PUT, DELETE endpoints Implemented action results (Ok, Created, BadRequest, NotFound) Structured code following RESTful conventions Successfully learned Entity Framework Core:\nDbContext configuration and setup Entity models and relationships Migrations for database schema management Query operations with LINQ Successfully implemented CRUD API endpoints:\nGET endpoints for retrieving single and multiple resources POST endpoints for creating new resources PUT endpoints for updating existing resources DELETE endpoints for removing resources Proper HTTP status codes and response formats Successfully tested and validated API:\nTested endpoints using Postman Implemented input validation with data annotations Added error handling and exception handling middleware Validated request data before processing Implemented proper HTTP status codes for different scenarios Gained ability to build production-ready REST APIs with ASP.NET Core.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Internship Overview This worklog documents the complete 12-week First Cloud Journey internship program. The program covers AWS cloud services, serverless architecture, and full-stack development with ASP.NET Core.\nDuration: September 8 - November 29, 2025 (12 weeks)\nProgram Focus: AWS Cloud Services + ASP.NET Core API Development\nWeek 1 (09/08 - 09/12): Getting familiar with AWS and basic services\nAWS fundamentals and service categories AWS Console and AWS CLI setup EC2 hands-on practice Week 2 (09/15 - 09/20): Learning AWS IAM and VPC\nIdentity and Access Management (IAM) Virtual Private Cloud (VPC) and networking Security groups and network configuration Week 3 (09/22 - 09/27): S3 and DynamoDB hands-on labs\nSimple Storage Service (S3) for object storage DynamoDB NoSQL database Storage and data management Week 4 (09/29 - 10/04): Lambda, API Gateway, and CloudWatch\nServerless computing with Lambda REST API creation with API Gateway Monitoring with CloudWatch Week 5 (10/06 - 10/11): Practice labs for previous services\nHands-on practice of IAM, VPC, S3, and DynamoDB Building integrated projects Troubleshooting and optimization Week 6 (10/13 - 10/18): Lambda, API Gateway, and CloudWatch practice\nServerless application development REST API integration and testing Application monitoring and logging Week 7 (10/20 - 10/25): CloudFormation and infrastructure automation\nInfrastructure as Code (IaC) with CloudFormation EC2 Auto Scaling and Load Balancing Multi-tier application deployment Week 8 (10/27 - 11/01): CI/CD pipelines and DevOps\nCodePipeline, CodeBuild, and CodeDeploy Automated deployment pipelines Distributed tracing with X-Ray Week 9 (11/03 - 11/08): ASP.NET Core fundamentals\nASP.NET Core project structure Dependency Injection and middleware Entity Framework Core REST API development Week 10 (11/10 - 11/15): ASP.NET Core with AWS integration\nJWT authentication and authorization Cognito integration for user management S3 and DynamoDB integration Secure API development Week 11 (11/17 - 11/22): Advanced ASP.NET Core features\nStructured logging with Serilog Caching strategies (in-memory and distributed) Swagger/OpenAPI documentation CloudWatch logging integration Week 12 (11/24 - 11/29): Video Sharing Platform project\nComplete application design and architecture Full-stack implementation with AWS services Deployment and CI/CD automation Production monitoring and optimization Program Outcomes By completing this 12-week internship, participants will have gained:\nComprehensive understanding of AWS cloud services and architecture Hands-on experience with Infrastructure as Code and DevOps practices Full-stack API development skills using ASP.NET Core Ability to design and deploy scalable, secure cloud applications Experience building real-world projects integrating multiple AWS services Knowledge of monitoring, logging, and operational excellence Technologies and Services Covered AWS Services:\nCompute: EC2, Lambda Storage: S3, EBS Database: DynamoDB, RDS Networking: VPC, API Gateway, CloudFront Security: IAM, Cognito DevOps: CloudFormation, CodePipeline, CodeBuild, CodeDeploy Monitoring: CloudWatch, X-Ray Development Stack:\nASP.NET Core framework Entity Framework Core JWT authentication Serilog logging Swagger/OpenAPI documentation "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/5-workshop/5.2-preparation/","title":"Preparation","tags":[],"description":"","content":"IAM Permission Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-northeast-1:833390817075:log-group:/aws/codebuild/CodeBuild-VideoShare\u0026#34;, \u0026#34;arn:aws:logs:ap-northeast-1:833390817075:log-group:/aws/codebuild/CodeBuild-VideoShare:*\u0026#34; ], \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::codepipeline-ap-northeast-1-*\u0026#34; ], \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codebuild:CreateReportGroup\u0026#34;, \u0026#34;codebuild:CreateReport\u0026#34;, \u0026#34;codebuild:UpdateReport\u0026#34;, \u0026#34;codebuild:BatchPutTestCases\u0026#34;, \u0026#34;codebuild:BatchPutCodeCoverages\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:codebuild:ap-northeast-1:833390817075:report-group/CodeBuild-VideoShare-*\u0026#34; ] } ] } "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/2-proposal/","title":"Proposal ","tags":[],"description":"","content":"Video Sharing Platform 1.Executive Summary This proposal outlines the development of a scalable Video Sharing Platform leveraging AWS cloud services. The platform will enable users to upload, stream, and share video content with features including user authentication, content management, and real-time video streaming.\nKey objectives:\nBuild a secure, scalable video sharing platform Implement user authentication and authorization Provide high-quality video streaming capabilities Ensure cost-effective infrastructure management Deliver seamless user experience across devices The solution utilizes AWS services including Amplify for frontend hosting, Cognito for authentication, S3 for storage, CloudFront for content delivery, and Interactive Video Service for streaming capabilities.\n2.Problem Statement What\u0026rsquo;s the Problem? Current video sharing solutions face several challenges:\nHigh infrastructure costs for video storage and streaming Complex setup and maintenance requirements Limited scalability during peak usage Security vulnerabilities in user authentication Poor video quality and buffering issues The Solution Our AWS-based video sharing platform addresses these challenges by:\nLeveraging AWS\u0026rsquo;s cost-effective, pay-as-you-use pricing model Utilizing managed services to reduce operational overhead Implementing auto-scaling capabilities for handling traffic spikes Providing enterprise-grade security through AWS Cognito Delivering high-quality video streaming via Amazon IVS and CloudFront Benefits and Return on Investment Cost Savings:\n40-60% reduction in infrastructure costs compared to traditional hosting No upfront hardware investments required Pay-per-use model optimizes operational expenses Performance Improvements:\n99.9% uptime availability Global content delivery with low latency Auto-scaling handles 10x traffic increases seamlessly Business Value:\nFaster time-to-market (3-6 months vs 12+ months) Enhanced user experience drives higher engagement Scalable architecture supports business growth 3.Solution Architecture AWS Services Used Amplify: Frontend hosting and deployment platform for React/Vue.js applications with CI/CD integration.\nCognito: User authentication and authorization service providing secure sign-up, sign-in, and access control.\nApp Runner: Containerized backend API hosting with automatic scaling and load balancing.\nDynamoDB: NoSQL database for storing user profiles, video metadata, and application data.\nS3: Object storage for video files, thumbnails, and static assets with versioning and lifecycle policies.\nCloudFront: Global CDN for fast content delivery and video streaming with edge caching.\nAmazon IVS (Interactive Video Service): Real-time video streaming service for live broadcasts and on-demand content with low latency.\nAWS Elemental MediaConvert: Used to turn original video into many formats, many resolutions, many codecs, for streaming or downloading.\nAWS Lambda: Serverless compute service for event-driven processing, video transcoding triggers, and backend automation tasks.\nCode Pipeline: CI/CD pipeline for automated testing, building, and deployment.\nCode Build: Build service for compiling source code, running tests, and creating deployment packages.\nElastic Container Registry: Docker container registry for storing and managing application images.\nComponent Design Frontend Layer:\nReact-based web application hosted on Amplify Responsive design supporting mobile and desktop Real-time video player with adaptive bitrate streaming API Layer:\nRESTful APIs built with .NET Containerized and deployed on App Runner AWS Lambda for serverless API endpoints JWT-based authentication integration Data Layer:\nDynamoDB tables for user data and video metadata S3 buckets for video storage with intelligent tiering Security Layer:\nCognito user pools for authentication IAM roles and policies for access control Video Processing Layer:\nAWS Lambda functions triggered by S3 events AWS Elemental MediaConvert for transcoding videos Automated conversion to multiple formats and resolutions HLS and DASH output for adaptive streaming Streaming Architecture:\nAmazon IVS for live streaming capabilities CloudFront for global video distribution Adaptive bitrate streaming for optimal quality Use Cases Live Streaming Events:\nReal-time broadcasting of conferences, webinars, and corporate events Multi-bitrate streaming for optimal viewer experience Video On Demand (VOD):\nUpload and share educational content, tutorials, and training materials Secure content access with user permissions Social Video Sharing:\nUser-generated content sharing Community features with comments and ratings 4.Technical Implementation Phase 1: Infrastructure Setup AWS Account Configuration:\nConfigure IAM roles and policies for least privilege access Core Services Deployment:\nDeploy DynamoDB tables with proper indexing Configure S3 buckets with encryption and lifecycle policies Set up Cognito user pools and identity pools Phase 2: Backend Development API Development:\nBuild RESTful APIs using Node.js/Express framework Implement JWT authentication with Cognito integration Create video upload/processing endpoints Develop Lambda functions for event-driven tasks Develop user management and content APIs Database Schema:\nUsers table: userId, avatarUrl, birthDate, channelId, createdAt, email, gender, name, phoneNumber Videos table: videoId, channelId, commentCount, createdAt, createdFromStreamId, description, duration, key, likeCount, playbackUrl, status, thumbnailUrl, title, type, updatedAt, userId, viewCount VideoLikes table: userId, videoId, createdAt Subscriptions table: userId, channelId, createdAt StreamSessions table: streamId, channelId, createdAt, description, endedAt, isLive, recordingUrl, startedAt, status, thumbnailUrl, title, updatedAt, userId, viewerCount Notifications table: recipientUserId, createdAt, actorAvatarUrl, actorName, actorUserId Comments table: videoId, commentId, content, createdAt, isDeleted, isEdited, likeCount, parentCommentId, replyCount, updatedAt, userAvatarUrl, userId, userName CommentLikes table: commentId, userId, createdAt Channels table: channelId, avatarUrl, channelArn, createdAt, currentStreamId, description, ingestEndpoint, isLive, name, playbackUrl, streamKeyArn, subscriberCount, userId, videoCount Containerization:\nCreate Docker containers for API services Push images to Elastic Container Registry Configure App Runner for automatic deployment Phase 3: Frontend Development React Application:\nImplement responsive UI components Integrate AWS Amplify SDK for authentication Build video upload interface with progress tracking Create video player with adaptive streaming Key Features:\nUser registration/login with email verification Video upload with drag-and-drop functionality Real-time video streaming with quality selection User dashboard for content management Phase 4: Streaming Integration AWS Lambda \u0026amp; MediaConvert Setup:\nCreate Lambda functions for S3 event handling Implement video processing workflow automation Configure MediaConvert job templates for transcoding Set up output presets (1080p, 720p, 480p) Generate HLS/DASH manifests for adaptive streaming Update DynamoDB with processing status Amazon IVS Setup:\nConfigure streaming channels and playback URLs Implement adaptive bitrate streaming Set up recording and archival workflows CloudFront Configuration:\nCreate distributions for video content delivery Configure edge locations for global reach Implement caching strategies for optimal performance Phase 5: CI/CD Pipeline Automated Deployment:\nConfigure CodePipeline for source-to-production workflow Set up CodeBuild for automated testing and building Implement blue-green deployment strategy Testing Strategy:\nUnit tests for API endpoints Integration tests for AWS service interactions Load testing for performance validation 5.Timeline \u0026amp; Milestones Project Duration: 8 Weeks (2 Months) Week 1: Setup \u0026amp; Planning\nAWS account setup and IAM configuration Project requirements finalization Team roles assignment Basic infrastructure deployment (S3, DynamoDB, Cognito) Week 2-3: Backend Development\nRESTful APIs with .NET JWT authentication with Cognito Video upload endpoints Database schemas implementation App Runner deployment Week 4-5: Frontend Development\nReact application with responsive design User authentication flows Video upload interface Basic video player Amplify deployment Week 6: Integration \u0026amp; Streaming\nFrontend-backend integration Lambda functions for video processing automation MediaConvert setup for video transcoding CloudFront setup for video delivery Basic streaming functionality Testing and bug fixes Week 7-8: Final Deployment\nProduction deployment User acceptance testing Documentation completion Project presentation preparation Key Milestones Milestone 1 (Week 1): Infrastructure Ready\nAWS services configured Development environment accessible Milestone 2 (Week 3): Backend Complete\nAPIs functional Authentication working Milestone 3 (Week 5): Frontend Complete\nUI fully developed Basic video upload/playback working Milestone 4 (Week 8): Production Launch\nSystem deployed and tested Documentation complete 6.Budget Estimation Monthly Operating Costs (USD) Compute Services:\nApp Runner (1 services): $5-15/month AWS Lambda: $0-2/month Amplify Hosting: $0-5/month Storage \u0026amp; Database:\nS3 Storage: $0-1/month DynamoDB: $0-2/month CloudFront Data Transfer: $0-2/month Streaming Services:\nAmazon IVS (100 hours/month): $150-300/month AWS Elemental MediaConvert: $20-50/month Security \u0026amp; Monitoring:\nCognito: $0/month CI/CD\nCodePipeline \u0026amp; CodeBuild: $1-3/month\nERC: $0-1/month\nCalculator\nTotal Monthly Cost: $17-44/month\n7.Risks Assessment Primary Risks Technical Risks:\nIntegration complexity → Start simple, gradually increase Time management → Build buffer time, prioritize core features Resource Risks:\nExceeding AWS Free Tier → Monitor usage, set up alerts Mitigation Solutions Technical Management:\nCloudFormation templates Phase-by-phase testing Dev/staging environments Contingency Plans:\nMVP: Basic video upload/playback Core: User auth + streaming Advanced: Live streaming (optional) Use AWS Educate credits Mock services for demos 8.Expected Outcomes Performance Metrics System Performance:\nVideo upload success rate: \u0026gt;95% Streaming latency: \u0026lt;3 seconds System uptime: \u0026gt;99% Concurrent users: 100+ Page load times: \u0026lt;2 seconds Success Criteria MVP Requirements:\nUser registration/login Basic video upload/playback Secure authentication Responsive interface System monitoring Stretch Goals:\nLive streaming capabilities Advanced analytics Social features Mobile companion appld buffer time, prioritize core features Resource Risks:\nExceeding AWS Free Tier → Monitor usage, set up alerts Mitigation Solutions Technical Management:\nCloudFormation templates Phase-by-phase testing Dev/staging environments Contingency Plans:\nMVP: Basic video upload/playback Core: User auth + streaming Advanced: Live streaming (optional) Use AWS Educate credits Mock services for demos 8.Expected Outcomes Performance Metrics System Performance:\nVideo upload success rate: \u0026gt;95% Streaming latency: \u0026lt;3 seconds System uptime: \u0026gt;99% Concurrent users: 100+ Page load times: \u0026lt;2 seconds Success Criteria MVP Requirements:\nUser registration/login Basic video upload/playback Secure authentication Responsive interface System monitoring Stretch Goals:\nLive streaming capabilities Advanced analytics Social features Mobile companion app Attachments / References Document\nVideo\nSlide\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Learn advanced ASP.NET Core features: authentication, authorization, API security. Integrate ASP.NET Core API with AWS services. Practic Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Learn about JWT authentication and Bearer tokens in ASP.NET Core 11/10/2025 11/10/2025 https://learn.microsoft.com/en-us/aspnet/core/security/authentication/jwt 2 Learn about authorization and role-based access control (RBAC) 11/11/2025 11/11/2025 https://learn.microsoft.com/en-us/aspnet/core/security/authorization/ 3 Practice: Implement JWT authentication in API, create login endpoint 11/12/2025 11/12/2025 https://learn.microsoft.com/en-us/aspnet/core/security/ 4 Learn about integrating ASP.NET Core with AWS services (Cognito, S3, DynamoDB) 11/13/2025 11/13/2025 https://docs.aws.amazon.com/sdk-for-net/ 5 Practice: Integrate API with AWS Cognito for user management 11/14/2025 11/14/2025 https://docs.aws.amazon.com/cognito/latest/developerguide/ 6 Practice: Integrate API with S3 for file upload/download and DynamoDB for data storage 11/15/2025 11/15/2025 https://docs.aws.amazon.com/sdk-for-net/ Week 10 Achievements: Successfully learned JWT (JSON Web Token) authentication:\nJWT structure and claims Token generation and validation Bearer token authorization Token expiration and refresh tokens Successfully implemented authentication in ASP.NET Core API:\nCreated login endpoint that generates JWT tokens Configured JWT validation middleware Protected endpoints with [Authorize] attribute Implemented secure password handling Successfully learned authorization and access control:\nRole-based access control (RBAC) Claims-based authorization Authorization policies and requirements Implementing role checks in endpoints Successfully implemented role-based authorization:\nCreated different user roles (Admin, User, etc.) Protected endpoints with role requirements Implemented authorization policies Tested access control with different roles Successfully integrated ASP.NET Core API with AWS Cognito:\nConfigured Cognito user pool in API Implemented user registration using Cognito SDK Implemented user login and JWT token generation Integrated Cognito for user management and authentication Successfully integrated ASP.NET Core API with AWS S3:\nUploaded files to S3 from API endpoints Downloaded files from S3 through API Managed S3 bucket operations from application Implemented proper error handling for S3 operations Successfully integrated ASP.NET Core API with AWS DynamoDB:\nCreated DynamoDB client in ASP.NET Core Implemented CRUD operations for DynamoDB Stored and retrieved data from DynamoDB tables Integrated database operations with API endpoints Successfully built complete application combining:\nASP.NET Core API for business logic JWT authentication with Cognito S3 integration for file storage DynamoDB integration for data persistence Proper error handling and validation CloudWatch logging for monitoring Gained ability to build enterprise-grade, secure APIs integrated with AWS services.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Learn advanced ASP.NET Core features: logging, caching, and performance optimization. Practice building scalable APIs with proper logging and monitoring. Learn about API documentation with Swagger/OpenAPI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Learn about logging frameworks: Serilog, NLog, and built-in logging in ASP.NET Core 11/17/2025 11/17/2025 https://learn.microsoft.com/en-us/aspnet/core/fundamentals/logging/ 2 Practice: Configure Serilog, implement structured logging, log to multiple sinks 11/18/2025 11/18/2025 https://serilog.net/ 3 Learn about caching: in-memory caching, distributed caching, cache strategies 11/19/2025 11/19/2025 https://learn.microsoft.com/en-us/aspnet/core/performance/caching/ 4 Practice: Implement in-memory cache and distributed caching in API 11/20/2025 11/20/2025 https://learn.microsoft.com/en-us/aspnet/core/performance/caching/distributed 5 Learn about API documentation: Swagger/OpenAPI, API documentation best practices 11/21/2025 11/21/2025 https://learn.microsoft.com/en-us/aspnet/core/tutorials/web-api-help-pages-using-swagger 6 Practice: Add Swagger documentation to API, document endpoints and models 11/22/2025 11/22/2025 https://swagger.io/ Week 11 Achievements: Successfully learned and implemented structured logging with Serilog in ASP.NET Core.\nConfigured logging to multiple sinks: Console, File, and AWS CloudWatch.\nImplemented in-memory and distributed caching strategies to improve API performance.\nAdded cache invalidation and TTL policies for data consistency.\nSuccessfully integrated Swagger/OpenAPI documentation into API.\nDocumented all API endpoints with XML comments and examples.\nTested API functionality through interactive Swagger UI.\nIntegrated comprehensive monitoring with CloudWatch for real-time log analysis.\nGained ability to build production-ready, well-documented, and monitored APIs.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Design and implement a complete Video Sharing Platform using AWS services and ASP.NET Core. Integrate all AWS services learned throughout the internship. Build a production-ready application with proper architecture and deployment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Design system architecture: Define components, AWS services, data flow 11/24/2025 11/24/2025 All previous weeks 2 Setup infrastructure: Create VPC, security groups, EC2 instances, RDS database 11/25/2025 11/25/2025 https://cloudjourney.awsstudygroup.com/ 3 Implement backend API: User management, authentication, video metadata endpoints 11/26/2025 11/26/2025 Previous ASP.NET Core weeks 4 Implement video operations: Upload to S3, store metadata in DynamoDB, Lambda processing 11/27/2025 11/27/2025 Weeks 3, 4, 6 5 Implement video delivery: Serve videos from S3, streaming, CloudFront CDN 11/28/2025 11/28/2025 https://docs.aws.amazon.com/cloudfront/ 6 Deploy and monitor: CloudFormation deployment, CloudWatch monitoring, testing 11/29/2025 11/29/2025 Week 7, 8 Week 12 Achievements: Successfully designed complete Video Sharing Platform architecture integrating all AWS services.\nImplemented user management system with JWT authentication and Cognito integration.\nBuilt video upload functionality with S3 integration and pre-signed URLs.\nCreated video metadata management system using DynamoDB with efficient querying.\nImplemented serverless video processing using Lambda triggered by S3 events.\nConfigured CloudFront CDN for efficient video delivery and streaming.\nSet up comprehensive monitoring and logging with CloudWatch and X-Ray.\nDeployed complete application using CloudFormation infrastructure as code.\nAutomated CI/CD pipeline with CodePipeline, CodeBuild, and CodeDeploy.\nSuccessfully integrated all AWS services learned throughout the internship into a production-ready application.\nGained comprehensive understanding of building, deploying, and monitoring enterprise-grade cloud applications.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/5-workshop/5.3-createrepository/","title":"Create Repository","tags":[],"description":"","content":" Open the Amazon ECR In the navigation pane, choose Repositories, then click Create repository: In the Create repository console: Enter name Repository Click Create After create "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Virtual Production on the Cloud: Grup Mediaprо Unleashes Creativity with AWS Virtual production is revolutionizing the media and entertainment industry. Grup Mediaprо, a leading Spanish production company, has successfully leveraged AWS cloud services to transform their production workflows and unlock new creative possibilities.\nBlog 2 - Contribute Content to AWS Using NDI with AWS Elemental MediaConnect Network Device Interface (NDI) technology has revolutionized how media content creators can share and distribute video feeds across networks. When combined with AWS Elemental MediaConnect, NDI enables seamless integration of remote content sources into AWS-based production workflows. This blog explores how organizations can leverage NDI and AWS services to create flexible, scalable media contribution workflows.\nBlog 3 - Dynamically configuring job settings with AWS Elemental MediaConvert AWS Elemental MediaConvert is a powerful video transcoding service that enables media organizations to convert video content into multiple formats optimized for different platforms and devices. While basic transcoding is straightforward, many production workflows require dynamic configuration of job settings based on input content characteristics, business rules, or customer requirements. This blog explores strategies for dynamically configuring MediaConvert jobs to create flexible, scalable video processing pipelines.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/5-workshop/5.4-createcodebuild/","title":"Create Code Build","tags":[],"description":"","content":" First, you need add file buildspec.yml in your source code with content like: version: 0.2 env: variables: AWS_DEFAULT_REGION: ap-northeast-1 # change your region ACCOUNT_ID: ############ # change your AWS account ID REPO_NAME: videoshare/repository # change your name repository phases: pre_build: commands: - echo Logging in to Amazon ECR... - aws --version - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com - REPOSITORY_URI=$ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$REPO_NAME - IMAGE_TAG=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7) - echo Repository URI $REPOSITORY_URI - echo Image Tag $IMAGE_TAG build: commands: - echo Building the Docker image for .NET API... - docker build -t $REPO_NAME:$IMAGE_TAG -f backend-video-sharing-platform/Dockerfile . - docker tag $REPO_NAME:$IMAGE_TAG $REPOSITORY_URI:latest post_build: commands: - echo Pushing Docker image to Amazon ECR... - docker push $REPOSITORY_URI:latest - echo Writing imagedefinitions.json for CodePipeline/App Runner... - printf \u0026#39;[{\u0026#34;name\u0026#34;:\u0026#34;%s\u0026#34;,\u0026#34;imageUri\u0026#34;:\u0026#34;%s\u0026#34;}]\u0026#39; \u0026#34;$REPO_NAME\u0026#34; \u0026#34;$REPOSITORY_URI:$IMAGE_TAG\u0026#34; \u0026gt; imagedefinitions.json - echo Build completed successfully! artifacts: files: - imagedefinitions.json Commit github\nOpen the Amazon CodeBuild\nIn the navigation pane, choose Build projects, then click Create project: In the Create project console:\nEnter project name Add Source 1 In field Source provider choose GitHub (If you not have Credential then you need connect it) Choose Repository In Buildspec choose Use a buildspec file Click Create build project "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"English Version During my internship, I participated in two significant events that enhanced my knowledge and skills in cloud technologies and DevOps practices.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: Saturday, November 15, 2025 | 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nSummary: Comprehensive workshop on AWS AI/ML services including Amazon SageMaker, Generative AI with Amazon Bedrock, prompt engineering techniques, RAG architecture, and Bedrock Agents. Gained hands-on experience with foundation models (Claude, Llama, Titan) and learned best practices for building production-ready AI applications.\nEvent 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: Monday, November 17, 2025 | 8:30 AM – 5:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nSummary: Full-day DevOps workshop covering CI/CD pipelines (CodeCommit, CodeBuild, CodeDeploy, CodePipeline), Infrastructure as Code (CloudFormation, AWS CDK), container services (Docker, ECR, ECS, EKS), and monitoring solutions (CloudWatch, X-Ray). Learned deployment strategies, automation best practices, and real-world case studies for DevOps transformations.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/5-workshop/5.5-createcodepipeline/","title":"Create Code PipeLine","tags":[],"description":"","content":" In the navigation pane, choose Pipelines, then click Create repository: In the Create new pipeline console choose Build custom pipeline: Enter name pipeline Choose source GitHub, create connection and choose repository Choose Other build providers Choose AWS CodeBuild Choose project created Choose skip test stage Choose skip deploy stage Create pipeline "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/5-workshop/5.6-testresult/","title":"Test Result","tags":[],"description":"","content":"Once the setup is complete, the automated CI/CD workflow operates as follows:\nCode Changes Pushed A developer commits and pushes new code to the source repository (GitHub or CodeCommit).\nPipeline Triggered CodePipeline automatically detects the change and starts the pipeline.\nSource Stage Executes CodePipeline fetches the latest version of the source code and passes it to the next stage. Build Stage (CodeBuild) CodeBuild pulls the source code, reads the buildspec.yml, logs in to ECR, builds the Docker image, tags it, and pushes it to the correct ECR repository. (you can see process in pipeline) Image Published to ECR The newly built Docker image is stored and versioned in Amazon ECR.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Amazon Web Services Vietnam Co., Ltd.] from [09/09/2025] to [12/12/2025], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [Cloud Developer], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Set clear daily schedules and track adherence to deadlines Develop better time management habits Improve problem-solving thinking Practice analytical thinking when facing challenges Learn from case studies and real-world examples Seek mentorship from experienced colleagues Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively Participate in presentation and public speaking opportunities Practice active listening and clear articulation of ideas Develop conflict resolution and negotiation skills "},{"uri":"https://ShiroSs00.github.io/fcj-workshop/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship?\nThe most satisfying aspect was seeing the direct impact of my work on real AWS projects. I particularly enjoyed collaborating with the team on cloud infrastructure solutions and receiving constructive feedback that helped me grow professionally. The opportunity to work with cutting-edge technologies while contributing meaningful results was incredibly rewarding.\nWhat do you think the company should improve for future interns?\nWhile the overall experience was excellent, I would suggest:\nEstablishing a more structured onboarding program for new interns Creating a dedicated internal wiki or knowledge base for quick reference Scheduling regular feedback sessions (bi-weekly or monthly) to track progress If recommending to a friend, would you suggest they intern here? Why or why not?\nAbsolutely, I would strongly recommend this internship program to friends. The company provides genuine learning opportunities, supportive mentors, and a collaborative team environment. Unlike some internships where interns are relegated to menial tasks, here you work on meaningful projects and feel valued as a team member. The combination of professional development and positive workplace culture makes this an exceptional internship experience.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nOrganize monthly technical workshops or knowledge-sharing sessions Create peer mentoring opportunities between interns and senior staff Develop a structured career guidance program to help interns plan their professional future Would you like to continue this program in the future?\nYes, I would be very interested in continuing with the FCJ program. If possible, I would like to deepen my expertise in AWS cloud architecture and contribute to more strategic projects.\nAny other comments (free sharing):\nOverall, this internship has been a transformative experience that exceeded my expectations. I gained not just technical skills but also valuable insights into professional workplace dynamics. I\u0026rsquo;m grateful for the opportunity and the trust the team placed in me. I look forward to applying what I\u0026rsquo;ve learned and potentially contributing to the team\u0026rsquo;s future projects.\n"},{"uri":"https://ShiroSs00.github.io/fcj-workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://ShiroSs00.github.io/fcj-workshop/tags/","title":"Tags","tags":[],"description":"","content":""}]